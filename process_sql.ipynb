{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from config import countries, dap_bidding_zones, interconnections\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "engine = create_engine(os.getenv(\"SQLALCHEMY_DATABASE_URI\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Node:\n",
    "    name: str\n",
    "    code: str\n",
    "    load_df: pd.DataFrame\n",
    "    gen_df: pd.DataFrame\n",
    "    dap_df: pd.DataFrame\n",
    "\n",
    "\n",
    "nodes = {\n",
    "    country_id: Node(\n",
    "        name=country_id,\n",
    "        code=domain,\n",
    "        load_df=None,\n",
    "        gen_df=None,\n",
    "        dap_df=None,\n",
    "    )\n",
    "    for country_id, domain in countries.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.12s/it]\n"
     ]
    }
   ],
   "source": [
    "for country_id, domain in tqdm(countries.items()):\n",
    "    load_df = pd.read_sql_query(\n",
    "        f\"\"\"SELECT \"DateTime\", \"TotalLoadValue\" \n",
    "        FROM load WHERE \"AreaCode\" = '{domain}' AND \"AreaTypeCode\" = 'CTY' AND \"DateTime\" >= '2015-01-01 00:00:00' AND \"DateTime\" <= '2020-01-01 00:00:00'\n",
    "        ORDER BY \"DateTime\";\"\"\",\n",
    "        engine,\n",
    "        parse_dates=[\"DateTime\"],\n",
    "        index_col=\"DateTime\",\n",
    "    )\n",
    "    assert load_df.index.is_unique\n",
    "    assert load_df.index.is_monotonic_increasing\n",
    "    load_df = load_df.resample(\"1h\").mean()\n",
    "    nodes[country_id].load_df = load_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:49<00:00, 22.98s/it]\n"
     ]
    }
   ],
   "source": [
    "for country_id, domain in tqdm(countries.items()):\n",
    "    gen_df = pd.read_sql_query(\n",
    "        f\"\"\"SELECT \"DateTime\", \"ProductionType\", \"ActualGenerationOutput\"\n",
    "        FROM generation WHERE \"AreaCode\" = '{domain}' AND \"AreaTypeCode\" = 'CTY' AND \"DateTime\" >= '2015-01-01 00:00:00' AND \"DateTime\" <= '2020-01-01 00:00:00'\n",
    "        ORDER BY \"DateTime\";\"\"\",\n",
    "        engine,\n",
    "        parse_dates=[\"DateTime\"],\n",
    "    )\n",
    "    gen_df = gen_df.pivot_table(\n",
    "        index=\"DateTime\", columns=\"ProductionType\", values=\"ActualGenerationOutput\"\n",
    "    )\n",
    "    assert gen_df.index.is_unique\n",
    "    assert gen_df.index.is_monotonic_increasing\n",
    "    gen_df = gen_df.resample(\"1h\").mean()\n",
    "    nodes[country_id].gen_df = gen_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "for country_id, domain in tqdm(countries.items()):\n",
    "    cty_dap_df = pd.DataFrame()\n",
    "    for bzn in dap_bidding_zones[country_id]:\n",
    "        if (\n",
    "            bzn == \"10Y1001A1001A59C\"\n",
    "        ):  # IE resolution changed midway through the dataset\n",
    "            query = f\"\"\"SELECT \"DateTime\", \"Price\", \"ResolutionCode\"\n",
    "            FROM dap WHERE \"AreaCode\" = '{bzn}' AND \"AreaTypeCode\" = 'BZN' AND \"DateTime\" >= '2015-01-01 00:00:00' AND \"DateTime\" <= '2020-01-01 00:00:00'\n",
    "            ORDER BY \"DateTime\";\"\"\"\n",
    "        else:\n",
    "            query = f\"\"\"SELECT \"DateTime\", \"Price\" \n",
    "            FROM dap WHERE \"AreaCode\" = '{bzn}' AND \"AreaTypeCode\" = 'BZN' AND \"ResolutionCode\" = 'PT60M' AND \"DateTime\" >= '2015-01-01 00:00:00' AND \"DateTime\" <= '2020-01-01 00:00:00'\n",
    "            ORDER BY \"DateTime\";\"\"\"\n",
    "        bzn_dap_df = pd.read_sql_query(\n",
    "            query,\n",
    "            engine,\n",
    "            parse_dates=[\"DateTime\"],\n",
    "            index_col=\"DateTime\",\n",
    "        )\n",
    "        if bzn == \"10Y1001A1001A59C\":\n",
    "            # pop index back into \"DateTime\" column\n",
    "            bzn_dap_df.reset_index(inplace=True)\n",
    "            bzn_dap_df = bzn_dap_df.sort_values(by=[\"DateTime\", \"ResolutionCode\"])\n",
    "            # for rows with the same DateTime, keep the row with ResolutionCode == \"PT60M\"\n",
    "            bzn_dap_df = bzn_dap_df.drop_duplicates(subset=\"DateTime\", keep=\"last\")\n",
    "            bzn_dap_df.drop(columns=[\"ResolutionCode\"], inplace=True)\n",
    "            bzn_dap_df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "        assert bzn_dap_df.index.is_unique\n",
    "        assert bzn_dap_df.index.is_monotonic_increasing\n",
    "        bzn_dap_df = bzn_dap_df.resample(\"1h\").mean()\n",
    "        cty_dap_df = pd.concat([cty_dap_df, bzn_dap_df], axis=1)\n",
    "    # Take the mean of the prices across bidding zones in the country\n",
    "    cty_dap_df = cty_dap_df.mean(axis=1)\n",
    "    cty_dap_df.columns = [\"Price\"]\n",
    "    cty_dap_df = cty_dap_df.to_frame()\n",
    "    nodes[country_id].dap_df = cty_dap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:25<00:00, 20.52s/it]\n"
     ]
    }
   ],
   "source": [
    "flow_df = pd.DataFrame()\n",
    "for country_id, neighbour_ids in tqdm(interconnections.items()):\n",
    "    from_domain = countries[country_id]\n",
    "    to_domains = [\n",
    "        (neighbour_id, countries[neighbour_id]) for neighbour_id in neighbour_ids\n",
    "    ]\n",
    "\n",
    "    for neighbour_id, to_domain in to_domains:\n",
    "        # print(f\"[{country_id}] Fetching flow data from {from_domain} to {to_domain}\")\n",
    "        ic_df = pd.read_sql_query(\n",
    "            f\"\"\"SELECT \"DateTime\", \"FlowValue\" \n",
    "            FROM flow WHERE \"OutAreaCode\" = '{from_domain}' AND \"InAreaCode\" = '{to_domain}' AND \"OutAreaTypeCode\" = 'CTY' AND \"InAreaTypeCode\" = 'CTY' AND \"DateTime\" >= '2015-01-01 00:00:00' AND \"DateTime\" <= '2020-01-01 00:00:00'\n",
    "            ORDER BY \"DateTime\";\"\"\",\n",
    "            engine,\n",
    "            parse_dates=[\"DateTime\"],\n",
    "            index_col=\"DateTime\",\n",
    "        )\n",
    "        assert ic_df.index.is_unique\n",
    "        assert ic_df.index.is_monotonic_increasing\n",
    "        ic_df = ic_df.resample(\"1h\").mean()\n",
    "        ic_df = ic_df.rename(columns={\"FlowValue\": f\"{country_id}->{neighbour_id}\"})\n",
    "        flow_df = pd.concat([flow_df, ic_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BE (43824, 1) (82072, 18) (43729, 1) (43824, 32)\n",
      "DK (43824, 1) (81328, 18) (43824, 1) (43824, 32)\n",
      "FR (43824, 1) (81729, 18) (43729, 1) (43824, 32)\n",
      "DE (43824, 1) (81330, 18) (43729, 1) (43824, 32)\n",
      "IE (43824, 1) (81343, 18) (43818, 1) (43824, 32)\n",
      "LU (43824, 1) (81330, 18) (43729, 1) (43824, 32)\n",
      "NL (43824, 1) (81954, 18) (43729, 1) (43824, 32)\n",
      "NO (43824, 1) (81761, 18) (43824, 1) (43824, 32)\n",
      "CH (43824, 1) (81353, 18) (43824, 1) (43824, 32)\n",
      "UK (43824, 1) (80906, 18) (43824, 1) (43824, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for country_id, domain in tqdm(countries.items()):\n",
    "    print(\n",
    "        country_id,\n",
    "        nodes[country_id].load_df.shape,\n",
    "        nodes[country_id].gen_df.shape,\n",
    "        nodes[country_id].dap_df.shape,\n",
    "        flow_df.shape,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Biomass', 'Fossil Brown coal/Lignite', 'Fossil Coal-derived gas', 'Fossil Gas', 'Fossil Hard coal', 'Fossil Oil', 'Hydro Pumped Storage', 'Hydro Run-of-river and poundage', 'Hydro Water Reservoir', 'Nuclear', 'Other', 'Other renewable', 'Solar', 'Waste', 'Wind Offshore', 'Wind Onshore', 'Geothermal', 'Fossil Peat']\n",
      "['Biomass', 'Fossil Gas', 'Fossil Hard coal', 'Fossil Oil', 'Solar', 'Waste', 'Wind Offshore', 'Wind Onshore', 'Hydro Run-of-river and poundage', 'Hydro Pumped Storage', 'Hydro Water Reservoir', 'Nuclear', 'Other', 'Other renewable', 'Fossil Brown coal/Lignite', 'Fossil Coal-derived gas', 'Geothermal', 'Fossil Peat']\n",
      "['Biomass', 'Fossil Gas', 'Fossil Hard coal', 'Fossil Oil', 'Hydro Pumped Storage', 'Hydro Run-of-river and poundage', 'Hydro Water Reservoir', 'Nuclear', 'Solar', 'Waste', 'Wind Offshore', 'Wind Onshore', 'Other', 'Other renewable', 'Fossil Brown coal/Lignite', 'Fossil Coal-derived gas', 'Geothermal', 'Fossil Peat']\n",
      "['Biomass', 'Fossil Brown coal/Lignite', 'Fossil Coal-derived gas', 'Fossil Gas', 'Fossil Hard coal', 'Fossil Oil', 'Geothermal', 'Hydro Pumped Storage', 'Hydro Run-of-river and poundage', 'Hydro Water Reservoir', 'Nuclear', 'Other', 'Other renewable', 'Solar', 'Waste', 'Wind Offshore', 'Wind Onshore', 'Fossil Peat']\n",
      "['Fossil Gas', 'Fossil Hard coal', 'Fossil Oil', 'Fossil Peat', 'Hydro Pumped Storage', 'Hydro Run-of-river and poundage', 'Other', 'Wind Onshore', 'Biomass', 'Solar', 'Waste', 'Hydro Water Reservoir', 'Nuclear', 'Wind Offshore', 'Other renewable', 'Fossil Brown coal/Lignite', 'Fossil Coal-derived gas', 'Geothermal']\n",
      "['Biomass', 'Fossil Gas', 'Hydro Run-of-river and poundage', 'Hydro Water Reservoir', 'Solar', 'Waste', 'Wind Onshore', 'Fossil Hard coal', 'Hydro Pumped Storage', 'Fossil Oil', 'Nuclear', 'Other', 'Wind Offshore', 'Other renewable', 'Fossil Brown coal/Lignite', 'Fossil Coal-derived gas', 'Geothermal', 'Fossil Peat']\n",
      "['Biomass', 'Fossil Gas', 'Fossil Hard coal', 'Hydro Run-of-river and poundage', 'Nuclear', 'Other', 'Solar', 'Waste', 'Wind Offshore', 'Wind Onshore', 'Hydro Pumped Storage', 'Fossil Oil', 'Hydro Water Reservoir', 'Other renewable', 'Fossil Brown coal/Lignite', 'Fossil Coal-derived gas', 'Geothermal', 'Fossil Peat']\n",
      "['Biomass', 'Fossil Gas', 'Hydro Pumped Storage', 'Hydro Run-of-river and poundage', 'Hydro Water Reservoir', 'Other', 'Other renewable', 'Waste', 'Wind Onshore', 'Solar', 'Fossil Hard coal', 'Fossil Oil', 'Nuclear', 'Wind Offshore', 'Fossil Brown coal/Lignite', 'Fossil Coal-derived gas', 'Geothermal', 'Fossil Peat']\n",
      "['Fossil Gas', 'Hydro Pumped Storage', 'Hydro Run-of-river and poundage', 'Hydro Water Reservoir', 'Nuclear', 'Solar', 'Wind Onshore', 'Biomass', 'Fossil Hard coal', 'Waste', 'Fossil Oil', 'Other', 'Wind Offshore', 'Other renewable', 'Fossil Brown coal/Lignite', 'Fossil Coal-derived gas', 'Geothermal', 'Fossil Peat']\n",
      "['Biomass', 'Fossil Gas', 'Fossil Hard coal', 'Fossil Oil', 'Hydro Pumped Storage', 'Hydro Run-of-river and poundage', 'Nuclear', 'Other', 'Solar', 'Wind Offshore', 'Wind Onshore', 'Waste', 'Hydro Water Reservoir', 'Other renewable', 'Fossil Brown coal/Lignite', 'Fossil Coal-derived gas', 'Geothermal', 'Fossil Peat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for country_id, domain in tqdm(countries.items()):\n",
    "    print(list(nodes[country_id].gen_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Biomass', 'Fossil Brown coal/Lignite', 'Fossil Coal-derived gas', 'Fossil Gas', 'Fossil Hard coal', 'Fossil Oil', 'Fossil Peat', 'Geothermal', 'Hydro Pumped Storage', 'Hydro Run-of-river and poundage', 'Hydro Water Reservoir', 'Nuclear', 'Other', 'Other renewable', 'Solar', 'Waste', 'Wind Offshore', 'Wind Onshore']\n"
     ]
    }
   ],
   "source": [
    "# Find the common column names of gen_df across all nodes\n",
    "common_gen_cols = set(nodes[\"BE\"].gen_df.columns)\n",
    "for node in nodes.values():\n",
    "    common_gen_cols = common_gen_cols.intersection(set(node.gen_df.columns))\n",
    "common_gen_cols = list(common_gen_cols)\n",
    "common_gen_cols.sort()\n",
    "print(common_gen_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "Biomass 10\n",
      "Fossil Brown coal/Lignite 10\n",
      "Fossil Coal-derived gas 10\n",
      "Fossil Gas 10\n",
      "Fossil Hard coal 10\n",
      "Fossil Oil 10\n",
      "Hydro Pumped Storage 10\n",
      "Hydro Run-of-river and poundage 10\n",
      "Hydro Water Reservoir 10\n",
      "Nuclear 10\n",
      "Other 10\n",
      "Other renewable 10\n",
      "Solar 10\n",
      "Waste 10\n",
      "Wind Offshore 10\n",
      "Wind Onshore 10\n",
      "Geothermal 10\n",
      "Fossil Peat 10\n"
     ]
    }
   ],
   "source": [
    "gen_cols = {}\n",
    "for country_id, domain in countries.items():\n",
    "    cols = list(nodes[country_id].gen_df.columns)\n",
    "    for col in cols:\n",
    "        if col not in gen_cols:\n",
    "            gen_cols[col] = 1\n",
    "        else:\n",
    "            gen_cols[col] += 1\n",
    "gen_cols = {\n",
    "    k: v for k, v in sorted(gen_cols.items(), key=lambda item: item[1], reverse=True)\n",
    "}\n",
    "print(len(gen_cols))\n",
    "for k, v in gen_cols.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country_id, domain in countries.items():\n",
    "    cols = list(nodes[country_id].gen_df.columns)\n",
    "    for gen_col in gen_cols.keys():\n",
    "        if gen_col not in cols:\n",
    "            nodes[country_id].gen_df[gen_col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the data into database\n",
    "for country_id, node in nodes.items():\n",
    "    node.load_df.to_sql(f\"{country_id}_load\", engine, if_exists=\"replace\")\n",
    "    node.gen_df.to_sql(f\"{country_id}_gen\", engine, if_exists=\"replace\")\n",
    "    node.dap_df.to_sql(f\"{country_id}_dap\", engine, if_exists=\"replace\")\n",
    "flow_df.to_sql(\"flow_32\", engine, if_exists=\"replace\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "euics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
