{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from config import (\n",
    "    countries,\n",
    "    dap_bidding_zones,\n",
    "    interconnections,\n",
    "    interconnections_edge_matrix,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import BatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "engine = create_engine(os.getenv(\"SQLALCHEMY_DATABASE_URI\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df = pd.read_sql_table(\"flow_32\", engine)\n",
    "flow_df = flow_df.set_index(\"DateTime\")\n",
    "flow_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_df = pd.DataFrame()\n",
    "for country_id in countries.keys():\n",
    "    dap_df[country_id] = pd.read_sql_table(f\"{country_id}_dap\", engine).set_index(\n",
    "        \"DateTime\"\n",
    "    )\n",
    "dap_df.index = pd.to_datetime(dap_df.index)\n",
    "dap_df.ffill(inplace=True)\n",
    "dap_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43729\n",
      "2015-01-04 23:00:00 2019-12-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "datetime_intersect = flow_df.index.intersection(dap_df.index)\n",
    "print(len(datetime_intersect))\n",
    "print(min(datetime_intersect), max(datetime_intersect))\n",
    "# Check if datetime_intersect is monotonically increasing\n",
    "assert all(\n",
    "    datetime_intersect[i] < datetime_intersect[i + 1]\n",
    "    for i in range(len(datetime_intersect) - 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df = flow_df.loc[datetime_intersect]\n",
    "dap_df = dap_df.loc[datetime_intersect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32)\n",
      "(43729, 2, 32)\n"
     ]
    }
   ],
   "source": [
    "edges = np.array(interconnections_edge_matrix)\n",
    "print(edges.shape)\n",
    "# Map edge names to indices\n",
    "edge_names = np.unique(edges)\n",
    "edge_map = {edge: i for i, edge in enumerate(edge_names)}\n",
    "edge_indices = np.array([edge_map[edge] for edge in edges.flatten()]).reshape(\n",
    "    edges.shape\n",
    ")\n",
    "# Repeat edge indices for each datetime\n",
    "edge_indices = np.repeat(\n",
    "    edge_indices[np.newaxis, :, :],\n",
    "    len(datetime_intersect),\n",
    "    axis=0,\n",
    ")\n",
    "print(edge_indices.shape)\n",
    "n_edges = edges.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43729, 32, 1)\n",
      "(43729, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Edge labels (flow) of shape (n_datetime, n_edges, 1)\n",
    "edge_labels = np.array(flow_df)\n",
    "# print(edge_labels.shape)\n",
    "edge_labels = np.reshape(\n",
    "    edge_labels, (len(datetime_intersect), edge_labels.shape[1], 1)\n",
    ")\n",
    "print(edge_labels.shape)\n",
    "edge_attributes = np.copy(edge_labels)\n",
    "print(edge_attributes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43729, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Node features (dap)\n",
    "node_features = np.array(dap_df)\n",
    "# print(node_features.shape)\n",
    "node_features = np.reshape(\n",
    "    node_features, (len(datetime_intersect), node_features.shape[1], 1)\n",
    ")\n",
    "print(node_features.shape)\n",
    "n_nodes = node_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    len(datetime_intersect)\n",
    "    == edge_indices.shape[0]\n",
    "    == edge_labels.shape[0]\n",
    "    == edge_attributes.shape[0]\n",
    "    == node_features.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-04 23:00:00\n",
      "[[0 0 0 0 3 3 3 4 4 4 4 2 2 2 2 2 5 6 6 7 7 7 7 7 8 8 1 1 9 9 9 9]\n",
      " [4 6 7 9 2 7 8 0 2 1 9 3 4 6 7 1 9 0 2 0 3 2 8 9 3 7 4 2 0 4 5 7]]\n",
      "[[   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [1315.79]\n",
      " [  52.  ]\n",
      " [ 617.  ]\n",
      " [ 279.  ]\n",
      " [1433.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [3205.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [2106.86]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [ 964.  ]\n",
      " [   0.  ]\n",
      " [ 704.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [ 169.19]\n",
      " [   0.  ]]\n",
      "[[   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [1315.79]\n",
      " [  52.  ]\n",
      " [ 617.  ]\n",
      " [ 279.  ]\n",
      " [1433.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [3205.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [2106.86]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [ 964.  ]\n",
      " [   0.  ]\n",
      " [ 704.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [   0.  ]\n",
      " [ 169.19]\n",
      " [   0.  ]]\n",
      "[[36.56 ]\n",
      " [28.625]\n",
      " [36.56 ]\n",
      " [22.34 ]\n",
      " [49.14 ]\n",
      " [22.34 ]\n",
      " [36.56 ]\n",
      " [28.538]\n",
      " [37.8  ]\n",
      " [42.86 ]]\n"
     ]
    }
   ],
   "source": [
    "# Print a snapshot of the graph data\n",
    "idx = 0\n",
    "print(datetime_intersect[idx])\n",
    "print(edge_indices[idx])\n",
    "print(edge_labels[idx])\n",
    "print(edge_attributes[idx])\n",
    "print(node_features[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATv2Conv.html\n",
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, hidden_channels, num_heads_GAT, dropout_p_GAT, edge_dim_GAT, momentum_GAT\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gat = GATv2Conv(\n",
    "            (-1, -1),\n",
    "            hidden_channels,\n",
    "            add_self_loops=False,\n",
    "            heads=num_heads_GAT,\n",
    "            edge_dim=edge_dim_GAT,\n",
    "        )\n",
    "        self.norm = BatchNorm1d(\n",
    "            hidden_channels,\n",
    "            momentum=momentum_GAT,\n",
    "            affine=False,\n",
    "            track_running_stats=False,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_p_GAT)\n",
    "\n",
    "    def forward(self, x, edge_indices, edge_attrs):\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm(x)\n",
    "        nodes_embedds = self.gat(x, edge_indices, edge_attrs)\n",
    "        nodes_embedds = F.leaky_relu(nodes_embedds, negative_slope=0.1)\n",
    "        return nodes_embedds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x + self.pe[: x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model,\n",
    "        num_heads_TR,\n",
    "        num_encoder_layers_TR,\n",
    "        num_decoder_layers_TR,\n",
    "        dropout_p_TR,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(dim_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads_TR,\n",
    "            num_decoder_layers=num_encoder_layers_TR,\n",
    "            num_encoder_layers=num_decoder_layers_TR,\n",
    "            dropout=dropout_p_TR,\n",
    "        )\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src = self.pos_encoder(src)\n",
    "        trg = self.pos_encoder(trg)\n",
    "        temporal_node_embeddings = self.transformer(src, trg)\n",
    "        return temporal_node_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeDecoder(nn.Module):\n",
    "    def __init__(self, hidden_channels, num_heads_GAT, num_edges, num_nodes):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(\n",
    "            num_nodes * hidden_channels * num_heads_GAT, hidden_channels\n",
    "        )\n",
    "        self.lin2 = nn.Linear(hidden_channels, num_edges)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the tensor\n",
    "        x = torch.flatten(x)\n",
    "        x = self.lin1(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.1)\n",
    "        x = self.lin2(x)\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels,\n",
    "        num_heads_GAT,\n",
    "        dropout_p_GAT,\n",
    "        edge_dim_GAT,\n",
    "        momentum_GAT,\n",
    "        dim_model,\n",
    "        num_heads_TR,\n",
    "        num_encoder_layers_TR,\n",
    "        num_decoder_layers_TR,\n",
    "        dropout_p_TR,\n",
    "        n_edges,\n",
    "        n_nodes,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(\n",
    "            hidden_channels, num_heads_GAT, dropout_p_GAT, edge_dim_GAT, momentum_GAT\n",
    "        )  # node embedding with GAT\n",
    "        self.transformer = Transformer(\n",
    "            dim_model,\n",
    "            num_heads_TR,\n",
    "            num_encoder_layers_TR,\n",
    "            num_decoder_layers_TR,\n",
    "            dropout_p_TR,\n",
    "        )\n",
    "        self.decoder = EdgeDecoder(hidden_channels, num_heads_GAT, n_edges, n_nodes)\n",
    "\n",
    "    def forward(self, x, edge_indices, edge_attrs):\n",
    "        src_embedds = []\n",
    "        for i in range(x.shape[0]):\n",
    "            src_embedds.append(self.encoder(x[i], edge_indices[i], edge_attrs[i]))\n",
    "        src_embedds = torch.stack(src_embedds)\n",
    "        trg_embedds = src_embedds[-1].unsqueeze(0)\n",
    "        temporal_node_embedds = self.transformer(src_embedds, trg_embedds)\n",
    "        temporal_node_embedds = temporal_node_embedds.squeeze(0)\n",
    "        edge_predictions = self.decoder(temporal_node_embedds)\n",
    "        return edge_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lemuelkl\\anaconda3\\envs\\euics\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model(\n",
    "    hidden_channels=32,\n",
    "    num_heads_GAT=2,\n",
    "    dropout_p_GAT=0.1,\n",
    "    edge_dim_GAT=1,  # edge attributes\n",
    "    momentum_GAT=0.1,\n",
    "    dim_model=32 * 2,  # hidden_channels * num_heads_GAT\n",
    "    num_heads_TR=2,\n",
    "    num_encoder_layers_TR=2,\n",
    "    num_decoder_layers_TR=2,\n",
    "    dropout_p_TR=0.1,\n",
    "    n_edges=n_edges,\n",
    "    n_nodes=n_nodes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, window_size, num_epochs, lr):\n",
    "    model = model.to(device)\n",
    "    data = [d.to(device) for d in data]\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        # for m in tqdm(range(len(data) - window_size)):\n",
    "        loss_sum = None\n",
    "        for m in range(len(data) - window_size):\n",
    "            optimizer.zero_grad()\n",
    "            x = torch.stack([data[m + i].x for i in range(window_size)])\n",
    "            edge_indices = torch.stack(\n",
    "                [data[m + i].edge_index for i in range(window_size)]\n",
    "            )\n",
    "            edge_attrs = torch.stack(\n",
    "                [data[m + i].edge_attr for i in range(window_size)]\n",
    "            )\n",
    "            y = data[m + window_size].y\n",
    "            y_pred = model(x, edge_indices, edge_attrs)\n",
    "            loss = criterion(y_pred, y)\n",
    "            # print(f\"Epoch {epoch}, Loss {loss.item()}\")\n",
    "            if loss_sum is None:\n",
    "                loss_sum = loss\n",
    "            else:\n",
    "                loss_sum += loss\n",
    "            if m % 24 * 7 == 0 or m == len(data) - window_size - 1:\n",
    "                diff = y.squeeze() - y_pred\n",
    "                diff = diff.detach().cpu().numpy()\n",
    "                print(f\"Epoch {epoch}, m={m}\", diff.mean())\n",
    "                loss_sum.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss_sum = None\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lemuelkl\\anaconda3\\envs\\euics\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, m=0 455.17493\n",
      "Epoch 0, m=24 445.8124\n",
      "Epoch 0, m=48 409.51834\n",
      "Epoch 0, m=72 359.78577\n",
      "Epoch 0, m=96 419.98407\n",
      "Epoch 0, m=120 446.94714\n",
      "Epoch 0, m=144 421.63446\n",
      "Epoch 0, m=168 400.60724\n",
      "Epoch 0, m=192 403.40442\n",
      "Epoch 0, m=216 400.99054\n",
      "Epoch 0, m=240 396.19318\n",
      "Epoch 0, m=264 392.6469\n",
      "Epoch 0, m=288 315.49377\n",
      "Epoch 0, m=312 377.28555\n",
      "Epoch 0, m=336 389.22055\n",
      "Epoch 0, m=360 361.6402\n",
      "Epoch 0, m=384 440.72812\n",
      "Epoch 0, m=408 348.7458\n",
      "Epoch 0, m=432 424.44574\n",
      "Epoch 0, m=456 358.2893\n",
      "Epoch 0, m=480 350.63287\n",
      "Epoch 0, m=504 392.09674\n",
      "Epoch 0, m=528 361.40875\n",
      "Epoch 0, m=552 427.5383\n",
      "Epoch 0, m=576 481.68613\n",
      "Epoch 0, m=600 506.83096\n",
      "Epoch 0, m=624 510.49695\n",
      "Epoch 0, m=648 512.7706\n",
      "Epoch 0, m=672 502.1629\n",
      "Epoch 0, m=696 416.30396\n",
      "Epoch 0, m=720 486.72626\n",
      "Epoch 0, m=744 485.39386\n",
      "Epoch 0, m=768 502.9057\n",
      "Epoch 0, m=792 530.1548\n",
      "Epoch 0, m=816 547.96326\n",
      "Epoch 0, m=840 524.4026\n",
      "Epoch 0, m=864 501.1123\n",
      "Epoch 0, m=888 519.0797\n",
      "Epoch 0, m=912 494.92566\n",
      "Epoch 0, m=936 531.9535\n",
      "Epoch 0, m=960 504.1588\n",
      "Epoch 0, m=984 466.2628\n",
      "Epoch 0, m=1008 497.1488\n",
      "Epoch 0, m=1032 517.49304\n",
      "Epoch 0, m=1056 447.5294\n",
      "Epoch 0, m=1080 511.27106\n",
      "Epoch 0, m=1104 510.7317\n",
      "Epoch 0, m=1128 475.21603\n",
      "Epoch 0, m=1152 543.1854\n",
      "Epoch 0, m=1176 509.47858\n",
      "Epoch 0, m=1200 523.9059\n",
      "Epoch 0, m=1224 453.15826\n",
      "Epoch 0, m=1248 536.15063\n",
      "Epoch 0, m=1272 406.79034\n",
      "Epoch 0, m=1296 452.55823\n",
      "Epoch 0, m=1320 422.39294\n",
      "Epoch 0, m=1344 494.06976\n",
      "Epoch 0, m=1368 414.64868\n",
      "Epoch 0, m=1392 370.02478\n",
      "Epoch 0, m=1416 456.74683\n",
      "Epoch 0, m=1440 476.72925\n",
      "Epoch 0, m=1464 510.1208\n",
      "Epoch 0, m=1488 585.9719\n",
      "Epoch 0, m=1512 548.6435\n",
      "Epoch 0, m=1536 525.82336\n",
      "Epoch 0, m=1560 439.58264\n",
      "Epoch 0, m=1584 443.77396\n",
      "Epoch 0, m=1608 457.98257\n",
      "Epoch 0, m=1632 514.37854\n",
      "Epoch 0, m=1656 554.3001\n",
      "Epoch 0, m=1680 580.7954\n",
      "Epoch 0, m=1704 477.49307\n",
      "Epoch 0, m=1728 470.7569\n",
      "Epoch 0, m=1752 423.60086\n",
      "Epoch 0, m=1776 515.4387\n",
      "Epoch 0, m=1800 510.97076\n",
      "Epoch 0, m=1824 441.88284\n",
      "Epoch 0, m=1848 449.09943\n",
      "Epoch 0, m=1872 446.82187\n",
      "Epoch 0, m=1896 439.85492\n",
      "Epoch 0, m=1920 454.1364\n",
      "Epoch 0, m=1944 488.06003\n",
      "Epoch 0, m=1968 475.19852\n",
      "Epoch 0, m=1992 388.365\n",
      "Epoch 0, m=2016 443.68048\n",
      "Epoch 0, m=2040 532.66125\n",
      "Epoch 0, m=2064 489.66617\n",
      "Epoch 0, m=2088 433.87003\n",
      "Epoch 0, m=2112 494.7095\n",
      "Epoch 0, m=2136 474.26968\n",
      "Epoch 0, m=2160 538.478\n",
      "Epoch 0, m=2184 542.27313\n",
      "Epoch 0, m=2208 467.1632\n",
      "Epoch 0, m=2232 434.0919\n",
      "Epoch 0, m=2256 412.48285\n",
      "Epoch 0, m=2280 419.8011\n",
      "Epoch 0, m=2304 373.54736\n",
      "Epoch 0, m=2328 424.4695\n",
      "Epoch 0, m=2352 412.78592\n",
      "Epoch 0, m=2376 411.07877\n",
      "Epoch 0, m=2400 348.9931\n",
      "Epoch 0, m=2424 371.27716\n",
      "Epoch 0, m=2448 419.43018\n",
      "Epoch 0, m=2472 428.30484\n",
      "Epoch 0, m=2496 411.5961\n",
      "Epoch 0, m=2520 442.16113\n",
      "Epoch 0, m=2544 343.0351\n",
      "Epoch 0, m=2568 322.1958\n",
      "Epoch 0, m=2592 398.2447\n",
      "Epoch 0, m=2616 390.99057\n",
      "Epoch 0, m=2640 404.18457\n",
      "Epoch 0, m=2664 412.11084\n",
      "Epoch 0, m=2688 337.3683\n",
      "Epoch 0, m=2712 426.56076\n",
      "Epoch 0, m=2736 444.87985\n",
      "Epoch 0, m=2760 367.7331\n",
      "Epoch 0, m=2784 382.79095\n",
      "Epoch 0, m=2808 368.27277\n",
      "Epoch 0, m=2832 332.54547\n",
      "Epoch 0, m=2856 432.94464\n",
      "Epoch 0, m=2880 377.50052\n",
      "Epoch 0, m=2904 426.05807\n",
      "Epoch 0, m=2928 454.46744\n",
      "Epoch 0, m=2952 376.48322\n",
      "Epoch 0, m=2976 390.12787\n",
      "Epoch 0, m=3000 406.9813\n",
      "Epoch 0, m=3024 427.8025\n",
      "Epoch 0, m=3048 433.2074\n",
      "Epoch 0, m=3072 393.33478\n",
      "Epoch 0, m=3096 381.93137\n",
      "Epoch 0, m=3120 367.43402\n",
      "Epoch 0, m=3144 388.55865\n",
      "Epoch 0, m=3168 417.43652\n",
      "Epoch 0, m=3192 440.73953\n",
      "Epoch 0, m=3216 399.4809\n",
      "Epoch 0, m=3240 385.79834\n",
      "Epoch 0, m=3264 385.00836\n",
      "Epoch 0, m=3288 500.36377\n",
      "Epoch 0, m=3312 427.989\n",
      "Epoch 0, m=3336 456.0655\n",
      "Epoch 0, m=3360 379.19604\n",
      "Epoch 0, m=3384 374.50867\n",
      "Epoch 0, m=3408 390.87225\n",
      "Epoch 0, m=3432 337.1958\n",
      "Epoch 0, m=3456 386.45813\n",
      "Epoch 0, m=3480 379.7989\n",
      "Epoch 0, m=3504 406.20135\n",
      "Epoch 0, m=3528 432.995\n",
      "Epoch 0, m=3552 352.75623\n",
      "Epoch 0, m=3576 362.57196\n",
      "Epoch 0, m=3600 441.845\n",
      "Epoch 0, m=3624 544.04834\n",
      "Epoch 0, m=3648 530.36096\n",
      "Epoch 0, m=3672 457.80698\n",
      "Epoch 0, m=3696 501.3739\n",
      "Epoch 0, m=3720 495.39206\n",
      "Epoch 0, m=3744 409.9418\n",
      "Epoch 0, m=3768 466.92902\n",
      "Epoch 0, m=3792 498.5011\n",
      "Epoch 0, m=3816 515.38196\n",
      "Epoch 0, m=3840 421.3944\n",
      "Epoch 0, m=3864 399.10278\n",
      "Epoch 0, m=3888 543.13544\n",
      "Epoch 0, m=3912 554.8489\n",
      "Epoch 0, m=3936 497.5301\n",
      "Epoch 0, m=3960 509.27167\n",
      "Epoch 0, m=3984 464.03394\n",
      "Epoch 0, m=4008 439.12738\n",
      "Epoch 0, m=4032 466.47058\n",
      "Epoch 0, m=4056 520.6658\n",
      "Epoch 0, m=4080 423.01205\n",
      "Epoch 0, m=4104 434.8775\n",
      "Epoch 0, m=4128 471.60663\n",
      "Epoch 0, m=4152 397.26685\n",
      "Epoch 0, m=4176 408.34586\n",
      "Epoch 0, m=4200 354.85233\n",
      "Epoch 0, m=4224 434.8278\n",
      "Epoch 0, m=4248 427.96927\n",
      "Epoch 0, m=4272 348.9947\n",
      "Epoch 0, m=4296 332.6651\n",
      "Epoch 0, m=4320 372.77386\n",
      "Epoch 0, m=4344 403.80252\n",
      "Epoch 0, m=4368 414.11688\n",
      "Epoch 0, m=4392 496.23306\n",
      "Epoch 0, m=4416 514.9664\n",
      "Epoch 0, m=4440 481.95886\n",
      "Epoch 0, m=4464 571.50354\n",
      "Epoch 0, m=4488 525.0447\n",
      "Epoch 0, m=4512 459.81183\n",
      "Epoch 0, m=4536 442.1958\n",
      "Epoch 0, m=4560 381.48615\n",
      "Epoch 0, m=4584 406.47498\n",
      "Epoch 0, m=4608 417.32193\n",
      "Epoch 0, m=4632 409.12048\n",
      "Epoch 0, m=4656 335.80902\n",
      "Epoch 0, m=4680 391.03275\n",
      "Epoch 0, m=4704 457.73657\n",
      "Epoch 0, m=4728 445.788\n",
      "Epoch 0, m=4752 473.75415\n",
      "Epoch 0, m=4776 501.01605\n",
      "Epoch 0, m=4800 452.64636\n",
      "Epoch 0, m=4824 430.45856\n",
      "Epoch 0, m=4848 430.63782\n",
      "Epoch 0, m=4872 518.74286\n",
      "Epoch 0, m=4896 459.0113\n",
      "Epoch 0, m=4920 472.1917\n",
      "Epoch 0, m=4944 439.3244\n",
      "Epoch 0, m=4968 437.4683\n",
      "Epoch 0, m=4992 411.23206\n",
      "Epoch 0, m=5016 450.93713\n",
      "Epoch 0, m=5040 428.1001\n",
      "Epoch 0, m=5064 413.7222\n",
      "Epoch 0, m=5088 411.93497\n",
      "Epoch 0, m=5112 475.14905\n",
      "Epoch 0, m=5136 529.38525\n",
      "Epoch 0, m=5160 525.26746\n",
      "Epoch 0, m=5184 515.00134\n",
      "Epoch 0, m=5208 535.11096\n",
      "Epoch 0, m=5232 496.838\n",
      "Epoch 0, m=5256 477.8396\n",
      "Epoch 0, m=5280 520.1081\n",
      "Epoch 0, m=5304 543.0255\n",
      "Epoch 0, m=5328 561.1064\n",
      "Epoch 0, m=5352 576.78784\n",
      "Epoch 0, m=5376 569.0095\n",
      "Epoch 0, m=5400 539.3715\n",
      "Epoch 0, m=5424 540.17163\n",
      "Epoch 0, m=5448 496.8281\n",
      "Epoch 0, m=5472 536.5445\n",
      "Epoch 0, m=5496 525.66724\n",
      "Epoch 0, m=5520 483.99588\n",
      "Epoch 0, m=5544 477.6468\n",
      "Epoch 0, m=5568 443.7207\n",
      "Epoch 0, m=5592 460.38904\n",
      "Epoch 0, m=5616 475.27084\n",
      "Epoch 0, m=5640 479.0185\n",
      "Epoch 0, m=5664 500.6728\n",
      "Epoch 0, m=5688 508.0055\n",
      "Epoch 0, m=5712 535.28\n",
      "Epoch 0, m=5736 540.2534\n",
      "Epoch 0, m=5760 507.0838\n",
      "Epoch 0, m=5784 523.4077\n",
      "Epoch 0, m=5808 548.84424\n",
      "Epoch 0, m=5832 585.111\n",
      "Epoch 0, m=5856 541.03015\n",
      "Epoch 0, m=5880 555.4447\n",
      "Epoch 0, m=5904 506.3916\n",
      "Epoch 0, m=5928 471.7438\n",
      "Epoch 0, m=5952 507.5763\n",
      "Epoch 0, m=5976 496.2806\n",
      "Epoch 0, m=6000 507.7843\n",
      "Epoch 0, m=6024 529.7501\n",
      "Epoch 0, m=6048 431.5541\n",
      "Epoch 0, m=6072 417.57867\n",
      "Epoch 0, m=6096 399.7837\n",
      "Epoch 0, m=6120 470.31485\n",
      "Epoch 0, m=6144 457.2656\n",
      "Epoch 0, m=6168 480.20364\n",
      "Epoch 0, m=6192 459.3227\n",
      "Epoch 0, m=6216 466.29974\n",
      "Epoch 0, m=6240 415.84747\n",
      "Epoch 0, m=6264 464.6354\n",
      "Epoch 0, m=6288 481.63507\n",
      "Epoch 0, m=6312 528.00385\n",
      "Epoch 0, m=6336 545.6322\n",
      "Epoch 0, m=6360 555.7887\n",
      "Epoch 0, m=6384 546.8378\n",
      "Epoch 0, m=6408 517.1109\n",
      "Epoch 0, m=6432 508.7074\n",
      "Epoch 0, m=6456 502.33722\n",
      "Epoch 0, m=6480 507.83862\n",
      "Epoch 0, m=6504 563.1084\n",
      "Epoch 0, m=6528 518.36523\n",
      "Epoch 0, m=6552 558.43\n",
      "Epoch 0, m=6576 534.05896\n",
      "Epoch 0, m=6600 516.39856\n",
      "Epoch 0, m=6624 488.7046\n",
      "Epoch 0, m=6648 575.9011\n",
      "Epoch 0, m=6672 485.1057\n",
      "Epoch 0, m=6696 466.27817\n",
      "Epoch 0, m=6720 490.25055\n",
      "Epoch 0, m=6744 479.79205\n",
      "Epoch 0, m=6768 469.82007\n",
      "Epoch 0, m=6792 513.45593\n",
      "Epoch 0, m=6816 577.5566\n"
     ]
    }
   ],
   "source": [
    "snapshots = []\n",
    "for i in range(len(datetime_intersect)):\n",
    "    x = torch.tensor(node_features[i], dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_indices[i], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_attributes[i], dtype=torch.float)\n",
    "    y = torch.tensor(edge_labels[i], dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    snapshots.append(data)\n",
    "print(len(snapshots))\n",
    "train(model, snapshots, window_size=24 * 4, num_epochs=100, lr=1e-9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "euics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
